{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+zZB9OoRsm/qzjcPXo5xX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fredyah/tudui-pytorch/blob/main/tudui_pytorch_WhatIs_nn_Optimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lh2IWLLIbOX_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import L1Loss, MSELoss, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Flatten, Linear\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 使用一個 nn.Module 做為 example\n",
        "\n",
        "class Tudui(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Tudui, self).__init__()\n",
        "    self.model1 = Sequential(\n",
        "      Conv2d(3, 32, 5, padding=2),\n",
        "      MaxPool2d(2),\n",
        "      Conv2d(32, 32, 5, padding=2),\n",
        "      MaxPool2d(2),\n",
        "      Conv2d(32, 64, 5, padding=2),\n",
        "      MaxPool2d(2),\n",
        "      Flatten(),\n",
        "      Linear(1024, 64),\n",
        "      Linear(64, 10)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.model1(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "oudu1XQVdRNh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = torchvision.datasets.CIFAR10(root=\"./dataset\", train=False, transform=torchvision.transforms.ToTensor(),\n",
        "                                       download=True)\n",
        "dataloader = DataLoader(dataset, batch_size=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtNtazNqdUvO",
        "outputId": "0845aa47-75f5-4cbb-9b66-58b09657ff70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./dataset/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 79718273.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./dataset/cifar-10-python.tar.gz to ./dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tudui = Tudui()\n",
        "\n",
        "optim = torch.optim.SGD(tudui.parameters(), lr=0.01)    ## 指定優化器為SGD, params 使用 tudui.parameters(), learning rate 設定為 0.01\n",
        "scheduler = StepLR(optim, step_size=5, gamma=0.1)  ## 使用 StepLR 排程調整 optim 的 learning rate ，每5輪調整一次，也就是 * 0.1\n",
        "## 梯度清零，還是一樣使用 optim 進行\n",
        "## 參數調優，就需要由 optim 改為 scheduler 進行\n",
        "\n",
        "\n",
        "loss_cross = CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(20):\n",
        "  running_loss = 0.0\n",
        "  for data in dataloader:\n",
        "    imgs, targets = data\n",
        "    outputs = tudui(imgs)\n",
        "    #print(outputs)\n",
        "    #print(targets)\n",
        "    result_loss = loss_cross(outputs, targets)\n",
        "    running_loss += result_loss\n",
        "    #print(result_loss)\n",
        "    optim.zero_grad()   ## 梯度清零 (這是因為 PyTorch 中的梯度是累加的（即每次調用 .backward() 時，計算的梯度會被加到已有的梯度中）。所以，在開始新的訓練步驟前，需要清除舊的梯度，避免累加錯誤的值)\n",
        "    result_loss.backward()  ## 調用 loss 的反相傳播，找出每個節點的梯度，這些梯度將用來更新模型的權重，以使損失最小化。\n",
        "    scheduler.step()   ## 對模行參數，進行調優。優化器根據計算出的梯度來調整模型的參數，使損失函數的值減少。常用的優化器包括 SGD（隨機梯度下降）、Adam 等\n",
        "  print(\"Running Loss =\", running_loss)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0RQoglydYKQ",
        "outputId": "45d8ff38-fc39-4c25-c2ce-5fe1f1c8e45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Loss = tensor(23058.5684, grad_fn=<AddBackward0>)\n",
            "Running Loss = tensor(23058.5684, grad_fn=<AddBackward0>)\n",
            "Running Loss = tensor(23058.5684, grad_fn=<AddBackward0>)\n",
            "Running Loss = tensor(23058.5684, grad_fn=<AddBackward0>)\n",
            "Running Loss = tensor(23058.5684, grad_fn=<AddBackward0>)\n",
            "Running Loss = tensor(23058.5684, grad_fn=<AddBackward0>)\n",
            "Running Loss = tensor(23058.5684, grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    }
  ]
}